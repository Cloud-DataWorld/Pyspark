{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf90964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d18932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(2)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254209c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|CurrentDate|\n",
      "+---+-----------+\n",
      "|  0| 2023-03-17|\n",
      "|  1| 2023-03-17|\n",
      "+---+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- CurrentDate: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#defaulut format of datetype yyyy-mm-dd\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "df1 = df.withColumn('CurrentDate',current_date())\n",
    "\n",
    "df1.show()\n",
    "\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc3e57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|CurrentDate|\n",
      "+---+-----------+\n",
      "|  0| 2023.00.17|\n",
      "|  1| 2023.00.17|\n",
      "+---+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- CurrentDate: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "df2 = df1.withColumn('CurrentDate', date_format(df1.CurrentDate, 'yyyy.mm.dd') )\n",
    "\n",
    "df2.show()\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ab8f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+\n",
      "| id|CurrentDate|    todate|\n",
      "+---+-----------+----------+\n",
      "|  0| 2023.00.17|2023-01-17|\n",
      "|  1| 2023.00.17|2023-01-17|\n",
      "+---+-----------+----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- CurrentDate: string (nullable = false)\n",
      " |-- todate: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df3 = df2.withColumn('todate', to_date(df2.CurrentDate,'yyyy.mm.dd'))\n",
    "\n",
    "df3.show()\n",
    "\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e90ba2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-------------------+\n",
      "| id|CurrentDate|    todate|CurrentDataDateType|\n",
      "+---+-----------+----------+-------------------+\n",
      "|  0| 2023.00.17|2023-01-17|         2023-01-17|\n",
      "|  1| 2023.00.17|2023-01-17|         2023-01-17|\n",
      "+---+-----------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- CurrentDate: string (nullable = false)\n",
      " |-- todate: date (nullable = true)\n",
      " |-- CurrentDataDateType: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4= df3.select('*',to_date(df3.CurrentDate,'yyyy.mm.dd').alias('CurrentDataDateType'))\n",
    "\n",
    "df4.show()\n",
    "\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79447e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|        d1|\n",
      "+----------+\n",
      "|2015-06-14|\n",
      "|2022-05-07|\n",
      "|2023-09-05|\n",
      "+----------+\n",
      "\n",
      "root\n",
      " |-- d1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "data = [('2015-06-14',),('2022-05-07',),('2023-09-05',),]\n",
    "\n",
    "schema = ['d1']\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7560b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|        d1|        d2|        d3|\n",
      "+----------+----------+----------+\n",
      "|2015-06-14|2022-05-07|2023-09-05|\n",
      "+----------+----------+----------+\n",
      "\n",
      "root\n",
      " |-- d1: string (nullable = true)\n",
      " |-- d2: string (nullable = true)\n",
      " |-- d3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "data = [('2015-06-14','2022-05-07','2023-09-05'),]\n",
    "\n",
    "schema = ['d1','d2','d3']\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "781e5bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+--------+\n",
      "|        d1|        d2|        d3|datediff|\n",
      "+----------+----------+----------+--------+\n",
      "|2015-06-14|2022-05-07|2023-09-05|    2519|\n",
      "+----------+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('datediff',datediff(df.d2,df.d1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d16f279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-------------+\n",
      "|        d1|        d2|        d3|monthsbetween|\n",
      "+----------+----------+----------+-------------+\n",
      "|2015-06-14|2022-05-07|2023-09-05|  82.77419355|\n",
      "+----------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between\n",
    "\n",
    "df.withColumn('monthsbetween',months_between(df.d2,df.d1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdcbcda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+\n",
      "|        d1|        d2|        d3| addmonths|\n",
      "+----------+----------+----------+----------+\n",
      "|2015-06-14|2022-05-07|2023-09-05|2022-10-07|\n",
      "+----------+----------+----------+----------+\n",
      "\n",
      "+----------+----------+----------+----------+\n",
      "|        d1|        d2|        d3| submonths|\n",
      "+----------+----------+----------+----------+\n",
      "|2015-06-14|2022-05-07|2023-09-05|2022-02-07|\n",
      "+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import add_months\n",
    "\n",
    "df.withColumn('addmonths',add_months(df.d2, 5)).show()\n",
    "\n",
    "df.withColumn('submonths',add_months(df.d2, -3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52a63466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+\n",
      "|        d1|        d2|        d3|   daysadd|\n",
      "+----------+----------+----------+----------+\n",
      "|2015-06-14|2022-05-07|2023-09-05|2015-06-17|\n",
      "+----------+----------+----------+----------+\n",
      "\n",
      "+----------+----------+----------+----------+\n",
      "|        d1|        d2|        d3|   subdays|\n",
      "+----------+----------+----------+----------+\n",
      "|2015-06-14|2022-05-07|2023-09-05|2015-06-19|\n",
      "+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "df.withColumn('daysadd',date_add(df.d1,3)).show()\n",
    "\n",
    "df.withColumn('subdays',date_add(df.d1, 5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50f948a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----+-----+\n",
      "|        d1|        d2|        d3|year|month|\n",
      "+----------+----------+----------+----+-----+\n",
      "|2015-06-14|2022-05-07|2023-09-05|2015|    6|\n",
      "+----------+----------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year,month\n",
    "\n",
    "df.withColumn('year',year(df.d1)) \\\n",
    "   .withColumn('month',month(df.d1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e1ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d724358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
