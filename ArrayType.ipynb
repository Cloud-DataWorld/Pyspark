{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c08872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1bdcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField,IntegerType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658d76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50b182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Numbers: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+---+-------+\n",
      "| ID|Numbers|\n",
      "+---+-------+\n",
      "|abc| [1, 2]|\n",
      "|xyz| [3, 4]|\n",
      "|pqr| [5, 6]|\n",
      "|efg| [7, 8]|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data= [('abc',[1,2]),('xyz',[3,4]),('pqr',[5,6]),('efg',[7,8])]\n",
    "\n",
    "schema = ['ID','Numbers']\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a4d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Numbers: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "+---+-------+\n",
      "| ID|Numbers|\n",
      "+---+-------+\n",
      "|abc| [1, 2]|\n",
      "|xyz| [3, 4]|\n",
      "|pqr| [5, 6]|\n",
      "|efg| [7, 8]|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Numbers: array<int>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType,StructType,StringType,IntegerType\n",
    "\n",
    "cust_schema = StructType([StructField('ID',StringType()),\\\n",
    "                           StructField('Numbers',ArrayType(IntegerType()))])\n",
    "\n",
    "df1 = spark.createDataFrame(data,cust_schema)\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "df1.show()\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c4ab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+\n",
      "| ID|Numbers|FirstNumber|\n",
      "+---+-------+-----------+\n",
      "|abc| [1, 2]|          1|\n",
      "|xyz| [3, 4]|          3|\n",
      "|pqr| [5, 6]|          5|\n",
      "|efg| [7, 8]|          7|\n",
      "+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn('FirstNumber',df1.Numbers[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec94779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+\n",
      "| ID|Numbers|FirstNumber|\n",
      "+---+-------+-----------+\n",
      "|abc| [1, 2]|          1|\n",
      "|xyz| [3, 4]|          3|\n",
      "|pqr| [5, 6]|          5|\n",
      "|efg| [7, 8]|          7|\n",
      "+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df1.withColumn('FirstNumber',col=col('Numbers')[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6d083b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num1: long (nullable = true)\n",
      " |-- num2: long (nullable = true)\n",
      "\n",
      "+----+----+\n",
      "|num1|num2|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   3|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,array\n",
    "\n",
    "data = [(1,2),(3,4)]\n",
    "\n",
    "schema = ['num1','num2']\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ea7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df.withColumn('numbers',array(col('num1'),col('num2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87cb91f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num1: long (nullable = true)\n",
      " |-- num2: long (nullable = true)\n",
      " |-- numbers: array (nullable = false)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+----+----+-------+\n",
      "|num1|num2|numbers|\n",
      "+----+----+-------+\n",
      "|   1|   2| [1, 2]|\n",
      "|   3|   4| [3, 4]|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.printSchema()\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explode()\n",
    "#split()\n",
    "#array()\n",
    "#array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "758948f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+---+--------+-------------+\n",
      "| ID|    Name|       Skills|\n",
      "+---+--------+-------------+\n",
      "|  1|srikanth|[python, AWS]|\n",
      "|  2| Manvith|  [Java, DWH]|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'srikanth',['python','AWS']),(2,'Manvith',['Java','DWH'])]\n",
    "\n",
    "schema = ['ID','Name','Skills']\n",
    "\n",
    "df7 = spark.createDataFrame(data,schema)\n",
    "\n",
    "df7.printSchema()\n",
    "\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b48bb98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+--------------+\n",
      "| ID|    Name|       Skills|Explode_Column|\n",
      "+---+--------+-------------+--------------+\n",
      "|  1|srikanth|[python, AWS]|        python|\n",
      "|  1|srikanth|[python, AWS]|           AWS|\n",
      "|  2| Manvith|  [Java, DWH]|          Java|\n",
      "|  2| Manvith|  [Java, DWH]|           DWH|\n",
      "+---+--------+-------------+--------------+\n",
      "\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Explode_Column: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,col\n",
    "\n",
    "df8= df7.withColumn('Explode_Column',col=explode(col('Skills')))\n",
    "\n",
    "df8.show()\n",
    "\n",
    "df8.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e9e5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------------+\n",
      "| ID|Name|          Skills|\n",
      "+---+----+----------------+\n",
      "|  1| sri|   .Net,Java,PHP|\n",
      "|  2|Manu|Python,AWS,Cloud|\n",
      "+---+----+----------------+\n",
      "\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Skills: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "data = [(1,'sri','.Net,Java,PHP'),(2,'Manu','Python,AWS,Cloud')]\n",
    "\n",
    "schema = ['ID','Name','Skills']\n",
    "\n",
    "df9 = spark.createDataFrame(data,schema)\n",
    "\n",
    "df9.show()\n",
    "\n",
    "df9.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f319e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------------+--------------------+\n",
      "| ID|Name|          Skills|            SplitCol|\n",
      "+---+----+----------------+--------------------+\n",
      "|  1| sri|   .Net,Java,PHP|   [.Net, Java, PHP]|\n",
      "|  2|Manu|Python,AWS,Cloud|[Python, AWS, Cloud]|\n",
      "+---+----+----------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Skills: string (nullable = true)\n",
      " |-- SplitCol: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df10 = df9.withColumn('SplitCol',col=split(col('Skills'),','))\n",
    "df10.show()\n",
    "df10.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "599d62f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------+--------------+\n",
      "| ID|Name|PrimarySkill|SecondarySkill|\n",
      "+---+----+------------+--------------+\n",
      "|  1| sri|        .Net|          Java|\n",
      "|  2|Manu|      Python|           AWS|\n",
      "+---+----+------------+--------------+\n",
      "\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- PrimarySkill: string (nullable = true)\n",
      " |-- SecondarySkill: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "data = [(1,'sri','.Net','Java'),(2,'Manu','Python','AWS')]\n",
    "\n",
    "schema = ['ID','Name','PrimarySkill','SecondarySkill']\n",
    "\n",
    "df11= spark.createDataFrame(data,schema)\n",
    "\n",
    "df11.show()\n",
    "\n",
    "df11.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae14cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------+--------------+-------------+\n",
      "| ID|Name|PrimarySkill|SecondarySkill|     ArrayCol|\n",
      "+---+----+------------+--------------+-------------+\n",
      "|  1| sri|        .Net|          Java| [.Net, Java]|\n",
      "|  2|Manu|      Python|           AWS|[Python, AWS]|\n",
      "+---+----+------------+--------------+-------------+\n",
      "\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- PrimarySkill: string (nullable = true)\n",
      " |-- SecondarySkill: string (nullable = true)\n",
      " |-- ArrayCol: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array\n",
    "\n",
    "df12= df11.withColumn('ArrayCol',col=array(col('PrimarySkill'),col('SecondarySkill')))\n",
    "\n",
    "df12.show()\n",
    "\n",
    "df12.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d5dff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| ID|    Name|       Skills|ArrayContains|\n",
      "+---+--------+-------------+-------------+\n",
      "|  1|srikanth|[python, AWS]|        false|\n",
      "|  2| Manvith|  [Java, DWH]|         true|\n",
      "+---+--------+-------------+-------------+\n",
      "\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Skills: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ArrayContains: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "data = [(1,'srikanth',['python','AWS']),(2,'Manvith',['Java','DWH'])]\n",
    "\n",
    "schema = ['ID','Name','Skills']\n",
    "\n",
    "df13 = spark.createDataFrame(data,schema)\n",
    "\n",
    "df14 = df13.withColumn('ArrayContains',col=array_contains(col('Skills'),'Java'))\n",
    "\n",
    "df14.show()\n",
    "\n",
    "df14.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f9817cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function array_contains in module pyspark.sql.functions:\n",
      "\n",
      "array_contains(col, value)\n",
      "    Collection function: returns null if the array is null, true if the array contains the\n",
      "    given value, and false otherwise.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    col : :class:`~pyspark.sql.Column` or str\n",
      "        name of column containing array\n",
      "    value :\n",
      "        value or column to check for in array\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "    >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "    [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "    >>> df.select(array_contains(df.data, lit(\"a\"))).collect()\n",
      "    [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(array_contains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb5e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
